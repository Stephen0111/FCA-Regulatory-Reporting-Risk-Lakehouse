{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc2b33a5-3fdc-4246-bcdd-aa3e4d3fed76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully created table: workspace.fca_regulatory.bronze_customer_master\n\uD83D\uDCCA Row count: 100\n\n\uD83D\uDCCA Sample data:\n+-----------+----------+---------+-------------+---------------+------------+\n|customer_id|first_name|last_name|date_of_birth|customer_status|created_date|\n+-----------+----------+---------+-------------+---------------+------------+\n|       C001|    First1|    Last1|   1968-01-21|     Vulnerable|  2024-01-01|\n|       C002|    First2|    Last2|   1999-04-06|        Dormant|  2024-01-01|\n|       C003|    First3|    Last3|   1969-03-25|         Active|  2024-01-01|\n|       C004|    First4|    Last4|   1960-08-02|     Vulnerable|  2024-01-01|\n|       C005|    First5|    Last5|   1956-02-27|         Active|  2024-01-01|\n+-----------+----------+---------+-------------+---------------+------------+\n\n\n\uD83D\uDCC1 Table location:\n+--------+---------+-------+\n|col_name|data_type|comment|\n+--------+---------+-------+\n|Location|         |       |\n+--------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Use the workspace catalog \n",
    "CATALOG = \"workspace\"\n",
    "SCHEMA = \"fca_regulatory\"\n",
    "\n",
    "# Create schema if it doesn't exist\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "\n",
    "# Generate customer data\n",
    "statuses = [\"Active\", \"Dormant\", \"Vulnerable\"]\n",
    "customers = [\n",
    "    (f\"C{i:03d}\", f\"First{i}\", f\"Last{i}\",\n",
    "     datetime(1950 + random.randint(0,50), random.randint(1,12), random.randint(1,28)),\n",
    "     random.choice(statuses), datetime(2024,1,1)) \n",
    "    for i in range(1,101)\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType()),\n",
    "    StructField(\"first_name\", StringType()),\n",
    "    StructField(\"last_name\", StringType()),\n",
    "    StructField(\"date_of_birth\", DateType()),\n",
    "    StructField(\"customer_status\", StringType()),\n",
    "    StructField(\"created_date\", DateType())\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df_customers = spark.createDataFrame(customers, customer_schema)\n",
    "\n",
    "# Write to managed table\n",
    "TABLE_NAME = f\"{CATALOG}.{SCHEMA}.bronze_customer_master\"\n",
    "df_customers.write.format(\"delta\").mode(\"overwrite\").saveAsTable(TABLE_NAME)\n",
    "\n",
    "print(f\"✅ Successfully created table: {TABLE_NAME}\")\n",
    "print(f\"\uD83D\uDCCA Row count: {df_customers.count()}\")\n",
    "\n",
    "# Verify the table was created\n",
    "print(\"\\n\uD83D\uDCCA Sample data:\")\n",
    "spark.sql(f\"SELECT * FROM {TABLE_NAME} LIMIT 5\").show()\n",
    "\n",
    "# Show table metadata\n",
    "print(\"\\n\uD83D\uDCC1 Table location:\")\n",
    "spark.sql(f\"DESCRIBE EXTENDED {TABLE_NAME}\").filter(\"col_name == 'Location'\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a403ee31-d827-47ca-a087-73a716d13b97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully created table: workspace.fca_regulatory.bronze_daily_transactions\n\uD83D\uDCCA Row count: 1000\n\n\uD83D\uDCCA Sample transactions:\n+--------------+-----------+----------------+----------------+--------+--------+\n|transaction_id|customer_id|transaction_date|transaction_type|  amount|currency|\n+--------------+-----------+----------------+----------------+--------+--------+\n|         T0001|       C082|      2024-01-11|         Deposit|18111.14|     GBP|\n|         T0002|       C080|      2024-01-25|         Deposit|16885.22|     EUR|\n|         T0003|       C100|      2024-01-23|         Deposit|10247.88|     USD|\n|         T0004|       C032|      2024-01-16|         Deposit|14207.33|     GBP|\n|         T0005|       C054|      2024-01-06|         Deposit| 8556.78|     GBP|\n|         T0006|       C077|      2024-01-25|         Deposit| 8461.97|     EUR|\n|         T0007|       C034|      2024-01-25|      Withdrawal| 2754.48|     GBP|\n|         T0008|       C076|      2024-01-16|            Loan|18850.46|     GBP|\n|         T0009|       C091|      2024-01-05|      Withdrawal| 1289.56|     EUR|\n|         T0010|       C011|      2024-01-06|         Deposit|12098.19|     GBP|\n+--------------+-----------+----------------+----------------+--------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Use Unity Catalog (same as before)\n",
    "CATALOG = \"workspace\"\n",
    "SCHEMA = \"fca_regulatory\"\n",
    "\n",
    "# Generate transaction data\n",
    "transaction_types = [\"Deposit\", \"Withdrawal\", \"Loan\"]\n",
    "currencies = [\"GBP\", \"USD\", \"EUR\"]\n",
    "transactions = [\n",
    "    (f\"T{i:04d}\", f\"C{random.randint(1,100):03d}\",\n",
    "     datetime(2024, 1, random.randint(1,28)),\n",
    "     random.choice(transaction_types),\n",
    "     round(random.uniform(50, 20000), 2),\n",
    "     random.choice(currencies)) \n",
    "    for i in range(1, 1001)\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "txn_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType()),\n",
    "    StructField(\"customer_id\", StringType()),\n",
    "    StructField(\"transaction_date\", DateType()),\n",
    "    StructField(\"transaction_type\", StringType()),\n",
    "    StructField(\"amount\", DoubleType()),\n",
    "    StructField(\"currency\", StringType())\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df_txns = spark.createDataFrame(transactions, txn_schema)\n",
    "\n",
    "# Write to Unity Catalog managed table (NOT using BRONZE_PATH)\n",
    "TABLE_NAME = f\"{CATALOG}.{SCHEMA}.bronze_daily_transactions\"\n",
    "df_txns.write.format(\"delta\").mode(\"overwrite\").saveAsTable(TABLE_NAME)\n",
    "\n",
    "print(f\"✅ Successfully created table: {TABLE_NAME}\")\n",
    "print(f\"\uD83D\uDCCA Row count: {df_txns.count()}\")\n",
    "\n",
    "# Verify the table was created\n",
    "print(\"\\n\uD83D\uDCCA Sample transactions:\")\n",
    "spark.sql(f\"SELECT * FROM {TABLE_NAME} LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1be438a1-ac62-4177-bf7f-7d19225f7c67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCA Simulating Bank of England market data...\n✅ Successfully created table: workspace.fca_regulatory.bronze_market_data\n\uD83D\uDCCA Row count: 365\n+----------+------------------+------------+------------+\n|      date|base_interest_rate|gbp_usd_rate|gbp_eur_rate|\n+----------+------------------+------------+------------+\n|2024-01-01|            5.2732|       1.264|      1.1737|\n|2024-01-02|            5.2157|      1.2526|      1.1677|\n|2024-01-03|            5.2198|      1.2795|      1.1803|\n|2024-01-04|            5.2312|      1.2719|      1.1537|\n|2024-01-05|            5.2217|      1.2859|      1.1607|\n|2024-01-06|            5.2798|      1.2824|       1.165|\n|2024-01-07|            5.2082|      1.2746|      1.1592|\n|2024-01-08|            5.2863|      1.2599|      1.1676|\n|2024-01-09|            5.2492|      1.2511|       1.156|\n|2024-01-10|            5.2791|      1.2831|      1.1645|\n+----------+------------------+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Use Unity Catalog\n",
    "CATALOG = \"workspace\"\n",
    "SCHEMA = \"fca_regulatory\"\n",
    "\n",
    "# Simulate Bank of England market data\n",
    "print(\"\uD83D\uDCCA Simulating Bank of England market data...\")\n",
    "\n",
    "start_date = datetime(2024, 1, 1)\n",
    "market_data = []\n",
    "base_rate = 5.25  # UK base rate\n",
    "\n",
    "for i in range(365):\n",
    "    date = start_date + timedelta(days=i)\n",
    "    interest_rate = round(base_rate + random.uniform(-0.05, 0.05), 4)\n",
    "    exchange_rate_usd = round(1.27 + random.uniform(-0.02, 0.02), 4)\n",
    "    exchange_rate_eur = round(1.17 + random.uniform(-0.02, 0.02), 4)\n",
    "    \n",
    "    market_data.append((date, interest_rate, exchange_rate_usd, exchange_rate_eur))\n",
    "\n",
    "market_schema = StructType([\n",
    "    StructField(\"date\", DateType()),\n",
    "    StructField(\"base_interest_rate\", DoubleType()),\n",
    "    StructField(\"gbp_usd_rate\", DoubleType()),\n",
    "    StructField(\"gbp_eur_rate\", DoubleType())\n",
    "])\n",
    "\n",
    "df_market = spark.createDataFrame(market_data, market_schema)\n",
    "\n",
    "TABLE_NAME = f\"{CATALOG}.{SCHEMA}.bronze_market_data\"\n",
    "df_market.write.format(\"delta\").mode(\"overwrite\").saveAsTable(TABLE_NAME)\n",
    "\n",
    "print(f\"✅ Successfully created table: {TABLE_NAME}\")\n",
    "print(f\"\uD83D\uDCCA Row count: {df_market.count()}\")\n",
    "\n",
    "spark.sql(f\"SELECT * FROM {TABLE_NAME} ORDER BY date LIMIT 10\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze_Data_Simulation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}